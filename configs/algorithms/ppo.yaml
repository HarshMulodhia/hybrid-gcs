# PPO Algorithm Configuration
# File: configs/algorithms/ppo.yaml

algorithm:
  name: "PPO"
  type: "policy_gradient"
  description: "Proximal Policy Optimization"

hyperparameters:
  learning_rate: 3e-4
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95  # GAE lambda
  entropy_coefficient: 0.01
  value_coefficient: 0.5
  max_grad_norm: 0.5

training:
  batch_size: 64
  mini_batch_size: 32
  num_epochs: 3
  num_steps: 2048  # Steps per update
  update_frequency: 2048
  num_envs: 4  # Parallel environments

network:
  policy_network:
    hidden_layers: [64, 64]
    activation: "relu"
    output_activation: "tanh"
  
  value_network:
    hidden_layers: [64, 64]
    activation: "relu"
    output_activation: "linear"

clipping:
  epsilon: 0.2  # PPO clip parameter
  value_clip: 0.2

optimization:
  optimizer: "adam"
  eps: 1e-5
  weight_decay: 0.0

regularization:
  l2_penalty: 1e-5
  dropout_rate: 0.0
  batch_norm: false

early_stopping:
  enabled: false
  patience: 10
  min_delta: 0.0

checkpointing:
  enabled: true
  frequency: 100  # episodes
  keep_best: true
  keep_last: 5
