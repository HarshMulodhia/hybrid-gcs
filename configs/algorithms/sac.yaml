# SAC Algorithm Configuration
# File: configs/algorithms/sac.yaml

algorithm:
  name: "SAC"
  type: "actor_critic"
  description: "Soft Actor-Critic"

hyperparameters:
  learning_rate:
    actor: 3e-4
    critic: 3e-4
    alpha: 3e-4
  
  gamma: 0.99  # Discount factor
  tau: 0.005   # Soft update coefficient
  alpha: 0.2   # Temperature coefficient
  target_entropy: null  # Auto-compute: -dim(action)

training:
  batch_size: 256
  replay_buffer_size: 1e6
  num_warmup_steps: 5000
  gradient_steps: 1
  num_envs: 1

networks:
  actor:
    hidden_layers: [256, 256]
    activation: "relu"
    output_activation: "tanh"
    squash_output: true
  
  critic:
    hidden_layers: [256, 256]
    activation: "relu"
    output_activation: "linear"
    num_critics: 2
  
  alpha_network:
    hidden_layers: [256, 256]
    activation: "relu"
    learnable: true

replay_buffer:
  type: "uniform"
  prioritized: false
  alpha: 0.6  # If prioritized
  beta: 0.4   # If prioritized

optimization:
  optimizer: "adam"
  eps: 1e-5
  weight_decay: 0.0

regularization:
  l2_penalty: 0.0
  dropout_rate: 0.0

target_update:
  method: "soft"
  tau: 0.005
  update_frequency: 1

action_scaling:
  enabled: true
  min_action: -1.0
  max_action: 1.0
